!pip install streamlit langchain langchain-community chromadb pymupdf sentence-transformers pyngrok --quiet

%%writefile app.py
import streamlit as st
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
import requests

# ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ù‡∏±‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (run ‡πÅ‡∏Ñ‡πà‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å)
@st.cache_resource
def load_vectorstore():
    with st.status("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å PDF...", expanded=True) as status:
        progress = st.progress(0, text="üìÑ Loading PDF...")
        
        pdf_path = "/content/‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏á‡∏≤‡∏ô medwaste (1).pdf"
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()
        progress.progress(20, text="‚úÇÔ∏è ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...")

        splitter = CharacterTextSplitter(separator="", chunk_size=1000, chunk_overlap=100)
        texts = splitter.split_documents(documents)
        for i, doc in enumerate(texts):
            doc.metadata["chunk_id"] = i
        documents = [doc.page_content for doc in texts if doc.metadata["chunk_id"] <= 274]
        progress.progress(50, text="üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (embedding)...")

        embedding = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={"device": "cpu"}
        )
        progress.progress(70, text="üíæ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡πÇ‡∏ï‡∏£‡πå...")

        vectordb = Chroma.from_texts(
            texts=documents,
            embedding=embedding,
            persist_directory="./chroma_db"
        )
        vectordb.persist()

        progress.progress(100, text="‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        status.update(label="‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‚úÖ", state="complete")
        return vectordb.as_retriever()

# Typhoon API
def ask_typhoon(chat_history):
    headers = {
        "Authorization": "Bearer sk-HQqPVR5RVGvTKVFcDHRJdVRtH3sQnH3VqKPHyYr5hoFsBFDj",
        "Content-Type": "application/json"
    }
    data = {
        "model": "typhoon-v2-70b-instruct",
        "messages": chat_history,
        "temperature": 0.5,
        "max_tokens": 512
    }
    res = requests.post("https://api.opentyphoon.ai/v1/chat/completions", headers=headers, json=data)
    return res.json()["choices"][0]["message"]["content"]

# ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
st.set_page_config(page_title="üìö RAG Chatbot - Medwaste", layout="wide")
st.title("üß™ ‡∏£‡∏∞‡∏ö‡∏ö RAG Chatbot ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡∏¢‡∏∞‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠")

# ‡πÉ‡∏™‡πà system prompt
system_prompt = "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏∞‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏• ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏∞‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏â‡∏±‡∏ô'"
retriever = load_vectorstore()
# ‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [{"role": "system", "content": system_prompt}]

user_input = st.text_input("üîç ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:")

if user_input:
    # RAG ‡∏Ç‡∏±‡πâ‡∏ô context
    docs = retriever.get_relevant_documents(user_input)
    selected_docs = docs[:3]
    context = "\n\n".join([doc.page_content for doc in selected_docs])

    st.markdown("### üìÑ Context ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:")
    for i, doc in enumerate(selected_docs):
        st.markdown(f"**Context {i+1}**: {doc.page_content.strip()}")

    prompt = f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n{context}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_input}"
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    response = ask_typhoon(st.session_state.chat_history)
    st.session_state.chat_history.append({"role": "assistant", "content": response})

    st.markdown("### ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢:")
    st.write(response)

# ‡∏õ‡∏¥‡∏î tunnel ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
ngrok.kill()
!pkill -f streamlit
from pyngrok import ngrok
import threading
import os

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Auth Token ‡∏Ç‡∏≠‡∏á ngrok
ngrok.set_auth_token("2ky4O7VCdeEAwsVQgws6wB6YjD5_2jjvwyMhCvfMFZK9og9wV")

# ‡πÄ‡∏õ‡∏¥‡∏î‡∏û‡∏≠‡∏£‡πå‡∏ï 8501 ‡∏ú‡πà‡∏≤‡∏ô ngrok
public_url = ngrok.connect(8501)
print(f"üåê ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏ó‡∏µ‡πà: {public_url}")

# ‡∏£‡∏±‡∏ô Streamlit ‡πÉ‡∏ô background
def run_app():
    os.system("streamlit run app.py")

thread = threading.Thread(target=run_app)
thread.start()

