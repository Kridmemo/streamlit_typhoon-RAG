import streamlit as st
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
import tempfile
import os
import requests

# ---- PAGE CONFIG ----
st.set_page_config(page_title="üìö RAG Chatbot - Typhoon", layout="wide")
st.title("üß™ ‡∏£‡∏∞‡∏ö‡∏ö RAG Chatbot ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå PDF")

# ---- UPLOAD PDF ----
uploaded_file = st.file_uploader("üì§ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏∞‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠", type=["pdf"])

# ---- SYSTEM PROMPT ----
default_prompt = "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏∞‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏• ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏∞‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏â‡∏±‡∏ô'"
system_prompt = st.text_area("üß† System Prompt", value=default_prompt, height=100)

if uploaded_file:
    with st.spinner("üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå..."):
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.read())
            pdf_path = tmp_file.name

        # Load & Split
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()

        splitter = CharacterTextSplitter(separator="", chunk_size=1000, chunk_overlap=100)
        texts = splitter.split_documents(documents)

        for i, doc in enumerate(texts):
            doc.metadata["chunk_id"] = i

        filtered_docs = [doc.page_content for doc in texts if doc.metadata["chunk_id"] <= 274]

        # ‡∏ù‡∏±‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå
        embedding = HuggingFaceEmbeddings(model_name="BAAI/bge-m3", model_kwargs={"device": "cpu"})
        vectordb = Chroma.from_texts(texts=filtered_docs, embedding=embedding)
        retriever = vectordb.as_retriever()

        st.success("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏ù‡∏±‡∏á PDF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß! ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á")

        # --- Session for chat ---
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = [{"role": "system", "content": system_prompt}]

        # --- Input Q&A ---
        user_input = st.chat_input("üí¨ ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")
        if user_input:
            with st.spinner("ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö..."):
                docs = retriever.get_relevant_documents(user_input)
                context = "\n\n".join([doc.page_content for doc in docs[:3]])

                st.session_state.chat_history.append({
                    "role": "user",
                    "content": f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n{context}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_input}"
                })

                def ask_typhoon(chat_history):
                    headers = {
                        "Authorization": "Bearer sk-HQqPVR5RVGvTKVFcDHRJdVRtH3sQnH3VqKPHyYr5hoFsBFDj",  # <-- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô token ‡∏ï‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á
                        "Content-Type": "application/json"
                    }
                    data = {
                        "model": "typhoon-v2-70b-instruct",
                        "messages": chat_history,
                        "temperature": 0.5,
                        "max_tokens": 512
                    }
                    res = requests.post("https://api.opentyphoon.ai/v1/chat/completions", headers=headers, json=data)
                    return res.json()["choices"][0]["message"]["content"]

                reply = ask_typhoon(st.session_state.chat_history)
                st.session_state.chat_history.append({"role": "assistant", "content": reply})

        # --- Display Chat History ---
        for message in st.session_state.chat_history:
            if message["role"] == "user":
                st.chat_message("user").markdown(message["content"])
            elif message["role"] == "assistant":
                st.chat_message("assistant").markdown(message["content"])
