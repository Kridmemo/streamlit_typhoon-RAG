import streamlit as st
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
import requests
import os
import threading
from langchain.vectorstores import FAISS
import base64

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏™‡∏µ‡∏ä‡∏°‡∏û‡∏π‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÇ‡∏•‡πÇ‡∏Å‡πâ
st.markdown(
    f"""
    <style>
    .stApp {{
        background-image: url("data:image/png;base64,{base64.b64encode(open('‡πÇ‡∏•‡πÇ‡∏Å‡πâ‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏û‡∏£‡∏´‡∏°‡∏Ñ‡∏µ‡∏£‡∏µ.png', 'rb').read()).decode()}");
        background-size: 150px;
        background-repeat: no-repeat;
        background-position: center right;
        
    }}
    </style>
    """,
    unsafe_allow_html=True
)

@st.cache_resource
def load_vectorstore():
    with st.status("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏Ç‡∏¢‡∏∞...", expanded=True) as status:
        progress = st.progress(0, text="üìÑ Loading PDF...")

        pdf_path = "‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏á‡∏≤‡∏ô medwaste (1).pdf"
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()
        progress.progress(20, text="‚úÇÔ∏è ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...")

        splitter = CharacterTextSplitter(separator="", chunk_size=1000, chunk_overlap=100)
        texts = splitter.split_documents(documents)
        for i, doc in enumerate(texts):
            doc.metadata["chunk_id"] = i
        documents = [doc.page_content for doc in texts if doc.metadata["chunk_id"] <= 274]
        progress.progress(50, text="üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (embedding)...")

        embedding = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={"device": "cpu"}
        )
        progress.progress(70, text="üíæ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡πÇ‡∏ï‡∏£‡πå...")

        vectordb = FAISS.from_texts(
            texts=documents,
            embedding=embedding
        )
        progress.progress(100, text="‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        status.update(label="‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‚úÖ", state="complete")
        return vectordb.as_retriever()

# Typhoon API
def ask_typhoon(chat_history):
    headers = {
        "Authorization": "Bearer sk-HQqPVR5RVGvTKVFcDHRJdVRtH3sQnH3VqKPHyYr5hoFsBFDj",
        "Content-Type": "application/json"
    }
    data = {
        "model": "typhoon-v2-70b-instruct",
        "messages": chat_history,
        "temperature": 0.5,
        "max_tokens": 512
    }
    res = requests.post("https://api.opentyphoon.ai/v1/chat/completions", headers=headers, json=data)
    return res.json()["choices"][0]["message"]["content"]

# ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
st.set_page_config(page_title="üóë RAG Chatbot - Medwaste", layout="wide")
st.title("üß™ ‡∏£‡∏∞‡∏ö‡∏ö RAG Chatbot ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏∞‡πÉ‡∏ô‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏û‡∏£‡∏´‡∏°‡∏Ñ‡∏µ‡∏£‡∏µ")

# ‡πÉ‡∏™‡πà system prompt
system_prompt = "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏∞‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏• ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏∞ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏â‡∏±‡∏ô'"
retriever = load_vectorstore()
# ‡πÄ‡∏Å‡πá‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [{"role": "system", "content": system_prompt}]

user_input = st.text_input("üîç ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:")

if user_input:
    # RAG ‡∏Ç‡∏±‡πâ‡∏ô context
    docs = retriever.get_relevant_documents(user_input)
    selected_docs = docs[:3]
    context = "\n\n".join([doc.page_content for doc in selected_docs])

    st.markdown("### üìÑ Context ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏Ç‡∏¢‡∏∞:")
    for i, doc in enumerate(selected_docs):
        st.markdown(f"**Context {i+1}**: {doc.page_content.strip()}")

    prompt = f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n{context}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {user_input}"
    st.session_state.chat_history.append({"role": "user", "content": prompt})
    response = ask_typhoon(st.session_state.chat_history)
    st.session_state.chat_history.append({"role": "assistant", "content": response})

    st.markdown("### ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢:")
    st.write(response)


# Add footer
st.markdown("""
    ---
    <div style='text-align: center;'>
        <p>‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏£‡∏≤‡∏£‡∏ñ‡∏ô‡∏≤‡∏î‡∏µ</p>
        <p>‡∏á‡∏≤‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° ‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏û‡∏£‡∏´‡∏°‡∏Ñ‡∏µ‡∏£‡∏µ</p>
    </div>
    """, unsafe_allow_html=True)
